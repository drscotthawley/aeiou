{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: routines for running on clusters\n",
    "output-file: hpc.html\n",
    "title: hpc\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part isn't strictly for audio i/o, but is nevertheless a normal part of Harmonai's operations. The point of this package is to reduce code-copying between Harmonai projects. \n",
    "\n",
    "**Heads up**: Huggingface `accelerate` support will likely be *deprecated* soon. We found `accelerate` necessary because of problems running PyTorch Lightning on multiple nodes, but those problems have now been resolved. Thus we will likely be using Lighting, so you will see that dependency being added and perhaps accelerate being removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### get_accel_config\n",
       "\n",
       ">      get_accel_config\n",
       ">                        (filename='~/.cache/huggingface/accelerate/default_conf\n",
       ">                        ig.yaml')\n",
       "\n",
       "get huggingface accelerate config info"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### get_accel_config\n",
       "\n",
       ">      get_accel_config\n",
       ">                        (filename='~/.cache/huggingface/accelerate/default_conf\n",
       ">                        ig.yaml')\n",
       "\n",
       "get huggingface accelerate config info"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|output: asis\n",
    "#| echo: false\n",
    "show_doc(get_accel_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'compute_environment': 'LOCAL_MACHINE',\n",
       " 'deepspeed_config': {},\n",
       " 'distributed_type': 'MULTI_GPU',\n",
       " 'fsdp_config': {},\n",
       " 'machine_rank': 0,\n",
       " 'main_process_ip': '',\n",
       " 'main_process_port': 12332,\n",
       " 'main_training_function': 'main',\n",
       " 'mixed_precision': 'no',\n",
       " 'num_machines': 2,\n",
       " 'num_processes': 8,\n",
       " 'use_cpu': False}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ac = get_accel_config('examples/accel_config.yaml')\n",
    "ac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is a little utility to replace `print`, where it'll only print on the cluster headnode. Note that you can only send one string to `hprint`, so use f-strings.  Also we use ANSI codes to color the text (currently cyan) to help it stand out from all the other text that's probably scrolling by!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### HostPrinter\n",
       "\n",
       ">      HostPrinter (accelerator, tag='\\x1b[96m', untag='\\x1b[0m')\n",
       "\n",
       "lil accelerate utility for only printing on host node\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| accelerator |  |  | huggingface accelerator object |\n",
       "| tag | str | \u001b[96m | starting color |\n",
       "| untag | str | \u001b[0m | reset to default color |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### HostPrinter\n",
       "\n",
       ">      HostPrinter (accelerator, tag='\\x1b[96m', untag='\\x1b[0m')\n",
       "\n",
       "lil accelerate utility for only printing on host node\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| accelerator |  |  | huggingface accelerator object |\n",
       "| tag | str | \u001b[96m | starting color |\n",
       "| untag | str | \u001b[0m | reset to default color |"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|output: asis\n",
    "#| echo: false\n",
    "show_doc(HostPrinter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "accelerator = accelerate.Accelerator()\n",
    "device = accelerator.device\n",
    "hprint = HostPrinter(accelerator)  # hprint only prints on head node\n",
    "hprint(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch+Accelerate Model routines\n",
    "For when the model is wrapped in a `accelerate` accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### save\n",
       "\n",
       ">      save (accelerator, args, model, opt=None, epoch=None, step=None)\n",
       "\n",
       "for checkpointing & model saves\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| accelerator |  |  | Huggingface accelerator object |\n",
       "| args |  |  | prefigure args dict, (we only use args.name) |\n",
       "| model |  |  | the model, pre-unwrapped |\n",
       "| opt | NoneType | None | optimizer state |\n",
       "| epoch | NoneType | None | training epoch number |\n",
       "| step | NoneType | None | training setp number |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### save\n",
       "\n",
       ">      save (accelerator, args, model, opt=None, epoch=None, step=None)\n",
       "\n",
       "for checkpointing & model saves\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| accelerator |  |  | Huggingface accelerator object |\n",
       "| args |  |  | prefigure args dict, (we only use args.name) |\n",
       "| model |  |  | the model, pre-unwrapped |\n",
       "| opt | NoneType | None | optimizer state |\n",
       "| epoch | NoneType | None | training epoch number |\n",
       "| step | NoneType | None | training setp number |"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|output: asis\n",
    "#| echo: false\n",
    "show_doc(save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### load\n",
       "\n",
       ">      load (accelerator, model, filename:str, opt=None)\n",
       "\n",
       "load a saved model checkpoint\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| accelerator |  |  | Huggingface accelerator object |\n",
       "| model |  |  | an uninitialized model (pre-unwrapped) whose weights will be overwritten |\n",
       "| filename | str |  | name of the checkpoint file |\n",
       "| opt | NoneType | None | optimizer state UNUSED FOR NOW |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### load\n",
       "\n",
       ">      load (accelerator, model, filename:str, opt=None)\n",
       "\n",
       "load a saved model checkpoint\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| accelerator |  |  | Huggingface accelerator object |\n",
       "| model |  |  | an uninitialized model (pre-unwrapped) whose weights will be overwritten |\n",
       "| filename | str |  | name of the checkpoint file |\n",
       "| opt | NoneType | None | optimizer state UNUSED FOR NOW |"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|output: asis\n",
    "#| echo: false\n",
    "show_doc(load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils for Accelerate or Lightning\n",
    "Be sure to use \"unwrap\" any accelerate model when calling these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### n_params\n",
       "\n",
       ">      n_params (module)\n",
       "\n",
       "Returns the number of trainable parameters in a module.\n",
       "Be sure to use accelerator.unwrap_model when calling this.\n",
       "\n",
       "|    | **Details** |\n",
       "| -- | ----------- |\n",
       "| module | raw PyTorch model/module, e.g. returned by accelerator.unwrap_model() |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### n_params\n",
       "\n",
       ">      n_params (module)\n",
       "\n",
       "Returns the number of trainable parameters in a module.\n",
       "Be sure to use accelerator.unwrap_model when calling this.\n",
       "\n",
       "|    | **Details** |\n",
       "| -- | ----------- |\n",
       "| module | raw PyTorch model/module, e.g. returned by accelerator.unwrap_model() |"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|output: asis\n",
    "#| echo: false\n",
    "show_doc(n_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### freeze\n",
       "\n",
       ">      freeze (model)\n",
       "\n",
       "freezes model weights; turns off gradient info\n",
       "If using accelerate, call thisaccelerator.unwrap_model when calling this.\n",
       "\n",
       "|    | **Details** |\n",
       "| -- | ----------- |\n",
       "| model | raw PyTorch model, e.g. returned by accelerator.unwrap_model() |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### freeze\n",
       "\n",
       ">      freeze (model)\n",
       "\n",
       "freezes model weights; turns off gradient info\n",
       "If using accelerate, call thisaccelerator.unwrap_model when calling this.\n",
       "\n",
       "|    | **Details** |\n",
       "| -- | ----------- |\n",
       "| model | raw PyTorch model, e.g. returned by accelerator.unwrap_model() |"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|output: asis\n",
    "#| echo: false\n",
    "show_doc(freeze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
